---
layout: post
title: "Good Performance for Bad Days"




related_posts:
  - "/2023/05/10/open-closed"
  - "/2021/08/05/utilization"
  - "/2021/08/27/caches"
---
{{ page.title }}
================

<script>
  MathJax = {
    tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
  };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<p class="meta">Good things are good, one finds.</p>

Two weeks ago, I flew to Toronto to give one of the keynotes at the International Conference on Performance Evaluation. It was fun. Smart people. Cool dark squirrels. Interesting conversations.

The core of what I tried to communicate is that, in my view, a lot of the performance evaluation community is overly focused on *happy case* performance (throughput, latency, scalability), and not focusing as much as we need to on performance under saturation and overload.

![](/blog/images/icpe_s1.png)

In fact, the opposite is potentially more interesting. For builders and operators of large systems, a lack of performance predictability under overload is a big driver of unavailability.

![](/blog/images/icpe_s2.png)

This is a common theme in postmortems and outage reports across the industry. Overload drives systems into regimes they aren't used to handling, which leads to downtime. Sometimes, in the case of [metastable failures](https://brooker.co.za/blog/2021/05/24/metastable.html), this leads to downtime that persists even after the overload has passed.

How did we get into this situation?

*Not Measuring the Hard Stuff*

At least one reason is immediately obvious if you pay attention to the performance evaluation in the majority of systems papers. Most of them show throughput, latency, or some other measure of goodness at a load far from the saturation point of the system.

![](/blog/images/icpe_s3.png)

The first-order reason for this is unsurprising: folks want to show the system they built in a good light. But there are some second-order reasons too. One is that performance evaluation is easiest, and most repeatable, in this part of the performance curve, and it takes expertise that many don't have to push beyond it.

Some bolder authors will compare saturation points, showing that their systems are able to do more good stuff even when the load is excessive.

![](/blog/images/icpe_s4.png)

Only the boldest will go beyond this saturation point to show the performance of their system under truly excessive amounts of load, after the point where performance starts to drop.

![](/blog/images/icpe_s5.png)

This regime is important, because it's very hard to compose reliable end-to-end systems without knowing where the saturation points of components are, and how they perform beyond that point. Even if you try do things like rate limiting and throttling at the front door, which you should, you still need to know how much you can send, and what the backend looks like when it starts saturating.

As a concrete example, TCP uses latency and loss as a signal to slow down, and assumes that if everybody slows down congestion will go away. These nice, clean, properties don't tend to be true of more complex systems.

*Open and Closed*

If you read only one performance-related systems paper in your life, make it [Open Versus Closed: A Cautionary Tale](https://www.usenix.org/legacy/event/nsdi06/tech/full_papers/schroeder/schroeder.pdf). This paper provides a crucial mental model for thinking about the performance of systems. Here's the key image from that paper:

![](/blog/images/open_closed.png)

When we look at the performance space, we see two things:

1. Most cloud systems are *open* (APIs, web sites, web services, MCP servers, whatever)
2. Most benchmarks are *closed* (TPC-C, YCSB, etc)

That doesn't make sense. The most famous downside of this disconnect is [coordinated omission](https://www.scylladb.com/2021/04/22/on-coordinated-omission/), where we massively underestimate the performance impact of tail latency. But that's far from the whole picture. Closed benchmarks are too kind to realistically reflect how performance changes with load, for the simple reason that they slow their load down when latency goes up.

![](/blog/images/icpe_s6.png)

The real world isn't that kind to systems. In most cases, if you slow down, you just have more work to be done later.

*Metastability*

As I've [written about before](https://brooker.co.za/blog/2021/05/24/metastable.html) ([a few times](https://brooker.co.za/blog/2019/05/01/emergent.html), [in different ways](https://brooker.co.za/blog/2022/06/02/formal.html)), metastability is a problem that distributed systems engineers need to pay more attention to. Not paying attention to performance under overload means not paying attention to metastability, where the majority of real-world triggers are overload-related.

Metastability isn't some esoteric problem. It can be triggered by retries.

<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/rvHd4Y76-fs?si=RsDPN1wbgksbTNo9&amp;start=149" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

or by [caches, as I've written about before](https://brooker.co.za/blog/2021/08/27/caches.html).

![](/blog/images/icpe_s7.png)

*Disconnect between benchmark and real-world workloads*

The other issue, as it always has been, is a disconnect between benchmark workloads and real-world workloads. This is true in that benchmarks don't reflect the size and scale of work done by real-world systems.

![](/blog/images/icpe_s9.png)

And in that they don't reflect the coordination and contention patterns present in real-world workloads.

![](/blog/images/icpe_s8.png)

The example I used was TPC-C, which has coordination patterns that are much easier to scale than most real-world workloads. [When visualized as a graph of rows that transact together](https://brooker.co.za/blog/2024/02/12/parameters.html), that becomes clear - you can basically partition on *warehouse* and avoid all cross-partition write-write conflicts.

*Conclusion*

Performance evaluation is super important to system designers and operators. This is a community I care about a lot. But I think there's a growing disconnect between practice and theory in this space, which we need to close to keep the field relevant and alive.